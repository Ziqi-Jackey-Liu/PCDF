import torch.nn as nn
import numpy as np
import Data_preprocessing
import torch


class CyclicVectorGenerator:
    def __init__(self, factor):
        """
        åˆå§‹åŒ–å‘é‡ç”Ÿæˆå™¨ã€‚
        :param factor: æ¯ä¸ªå‘¨æœŸä¸­å…ƒç´ ä¹‹é—´çš„æ¯”ä¾‹å› å­ã€‚
        """
        self.factor = factor

    @staticmethod
    def generate_periodic_vector(period, num_vectors):
        """
        ç”Ÿæˆå•ä¸ªå‘¨æœŸçš„å‘é‡ï¼Œå…ƒç´ ä¸ºéšæœºå¤§å°ï¼Œå½’ä¸€åŒ–ä¸º1ã€‚
        """
        # periodic_vector = torch.randn(period)  # å•å‘¨æœŸå†…å…ƒç´ ä¸ºéšæœºå¤§å°
        # periodic_vector /= periodic_vector.sum()  # å½’ä¸€åŒ–

        random_matrix = torch.randn(period, num_vectors)
        orthogonal_vectors, _ = torch.linalg.qr(random_matrix, mode='reduced')
        return orthogonal_vectors.T

    def generate_final_vector(self, length, period, num_vectors):
        """
        ç”Ÿæˆç›®æ ‡é•¿åº¦çš„å‘é‡ï¼Œæ»¡è¶³æ¯ä¸ªå‘¨æœŸçš„å…ƒç´ å€¼æ¯”å‰ä¸€ä¸ªå‘¨æœŸå°factorå€ã€‚
        """
        periodic_vector = self.generate_periodic_vector(period, num_vectors)
        vectors = []

        # å¾ªç¯æ‹¼æ¥å‘¨æœŸï¼ŒæŒ‰ç…§æ¯”ä¾‹é€’å‡
        current_vector = periodic_vector.clone()
        total_length = 0
        while total_length < length:
            vectors.append(current_vector)
            total_length += period
            current_vector = current_vector * self.factor  # ç¼©å°må€

        # æ‹¼æ¥æ‰€æœ‰å®Œæ•´å‘¨æœŸ
        final_vector = torch.cat(vectors, dim=1)[:, :length]  # æˆªå–åˆ°ç›®æ ‡é•¿åº¦

        # # å½’ä¸€åŒ–ä»¥ç¡®ä¿æ‰€æœ‰å…ƒç´ å’Œä¸º1
        # final_vector /= final_vector.sum(dim=1, keepdim=True)
        # final_vector = final_vector - final_vector.mean(dim=1, keepdim=True)
        return periodic_vector, final_vector


class CyclicVectorGeneratorD(nn.Module):
    def __init__(self):
        super(CyclicVectorGeneratorD, self).__init__()

    @staticmethod
    def generate_circular_periodic_vector(period):
        """
        é€šè¿‡ä¸¤æ¬¡ FFT ç”Ÿæˆæ­£äº¤çš„å¾ªç¯çŸ©é˜µã€‚
        """
        # ç”Ÿæˆéšæœºå‘é‡
        random_vector = torch.randn(period, dtype=torch.complex64)

        # ç¬¬ä¸€æ¬¡ FFT
        random_vector_fft = torch.fft.fft(random_vector)

        # è°ƒæ•´æ¨¡é•¿ä¸º 1
        random_vector_fft_normalized = random_vector_fft / torch.abs(random_vector_fft)

        # åå‘ FFT å¾—åˆ°æ­£äº¤å¾ªç¯çŸ©é˜µçš„ç¬¬ä¸€åˆ—
        c_prime = torch.fft.ifft(random_vector_fft_normalized).real

        # åˆ›å»ºæ­£äº¤çš„å¾ªç¯çŸ©é˜µï¼ˆæ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªå‘¨æœŸå‘é‡ï¼‰
        cyclic_matrix = torch.stack([torch.roll(c_prime, i) for i in range(period)])

        # è½¬ç½®çŸ©é˜µï¼Œä½¿æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ­£äº¤å‘é‡
        # cyclic_matrix = (cyclic_matrix - cyclic_matrix.min()) / (cyclic_matrix.max() - cyclic_matrix.min())
        return cyclic_matrix.T

    @staticmethod
    def generate_non_periodic_vector(period, num_vectors=1):
        return torch.randn(period, period)

    def forward(self, period, mode):
        if mode == 'periodic':
            periodic_vector = self.generate_circular_periodic_vector(period)
        else:
            periodic_vector = self.generate_non_periodic_vector(period)
        return periodic_vector


class CircularConvolution1(nn.Module):
    def __init__(self):
        super(CircularConvolution1, self).__init__()
        self.none = None

    def forward(self, key, input_x, factor):
        none = self.none
        n = input_x.size(0)  # è¾“å…¥å‘é‡çš„é•¿åº¦
        m = key.size(0)      # å·ç§¯æ ¸çš„é•¿åº¦

        # åˆå§‹åŒ–ç»“æœå¼ é‡ï¼Œå½¢çŠ¶ä¸ input_x ç›¸åŒ
        result = torch.zeros_like(input_x)
        key_values = []

        # éå†æ¯ä¸ªå…ƒç´ ä½ç½®è¿›è¡Œå¾ªç¯å·ç§¯
        for j in range(n):
            temp_key = []
            for k in range(m):  # input_x.size(0)
                result[j] += input_x[k] * key[(j - k) % m]
                temp_key.append(key[(j - k) % m].item())
            # key = torch.multiply(key, factor)  # æ¯æ¬¡å·ç§¯åæ”¾å¤§ k å€
            key_values.append(temp_key)
        return result, torch.tensor(key_values)


class CircularConvolution(nn.Module):
    def __init__(self):
        super(CircularConvolution, self).__init__()
        self.none = None

    def forward(self, key, input_x, factor):
        none = self.none
        n = input_x.size(0)  # è¾“å…¥å‘é‡çš„é•¿åº¦
        m = key.size(0)      # å·ç§¯æ ¸çš„é•¿åº¦

        # åˆå§‹åŒ–ç»“æœå¼ é‡ï¼Œå½¢çŠ¶ä¸ input_x ç›¸åŒ
        result = torch.zeros_like(input_x)
        key_values = []

        # éå†æ¯ä¸ªå…ƒç´ ä½ç½®è¿›è¡Œå¾ªç¯å·ç§¯
        for j in range(n):
            temp_key = []
            for k in range(m):  # éå†å·ç§¯æ ¸çš„å…ƒç´ 
                result[j] += input_x[(j + k) % n] * key[k]
                temp_key.append(key[k].item())
            key_values.append(temp_key)
            key = torch.multiply(key, factor)  # æ¯æ¬¡å·ç§¯åæ”¾å¤§ k å€
        # for k in range(input_x.size(0)):  # input_x.size(0)
        #     result[j] += input_x[k] * key[(j + input_x.size(0) - k) % key.size(0)]
        #     key_index = (j + input_x.size(0) - k) % key.size(0)
        #     temp_key.append(key[key_index].item())
        # key_values.append(temp_key)
        return result, torch.tensor(key_values)


class CircularCorrelation(nn.Module):
    def __init__(self):
        super(CircularCorrelation, self).__init__()
        self.none = None
        self.key_matrix1 = None
        self.key_matrix2 = None

    def forward(self, key, input_x, key_matrix1, input_o1, key_matrix2, input_o2):
        self.key_matrix1 = key_matrix1
        self.key_matrix2 = key_matrix2
        """
        Perform circular convolution between vectors c and x.
        Args:
            key: torch.Tensor, first vector of shape (1, 3n)
            input_x: torch.Tensor, second vector of shape (1, n)
        Returns:
            torch.Tensor: Result of circular convolution of shape (1, n)
        """
        self.none = None
        n = input_x.size(0)  # è¾“å…¥å‘é‡çš„é•¿åº¦
        m = key.size(0)  # å·ç§¯æ ¸çš„é•¿åº¦
        key_values = []

        result = torch.zeros_like(input_x)
        for j in range(n):
            temp_key = []
            for k in range(m):
                result[j] += input_x[k] * key[(k - j) % m]
                temp_key.append(key[(k - j) % m].item())
            key_values.append(temp_key)
            key_current = torch.tensor(torch.tensor(temp_key))
            redun1, redun2 = self.coefficient(self.key_matrix1, key_current, input_o1, j)
            redun3, redun4 = self.coefficient(self.key_matrix2, key_current, input_o2, j)
            result[j] = torch.div(result[j] - redun2 - redun4, redun1)
        # for j in range(input_x.size(0)):
        #     for k in range(input_x.size(0)):
        #         result[j] += input_x[k] * key[(k - j + input_x.size(0)) % key.size(0)]
        return result, redun3

    @staticmethod
    def summ(vector, window_size=20):
        squared_sums = torch.conv1d(vector.view(1, 1, -1) ** 2, torch.ones(1, 1, window_size), stride=1).view(-1)
        return torch.flip(squared_sums, dims=[0])

    @staticmethod
    def coefficient(key_matrix, key_current, input_x, index):
        input_x = torch.cat((input_x[:index], input_x[index + 1:]))
        r_1 = torch.mv(key_matrix, key_current)
        r_3 = torch.cat((r_1[:index], r_1[index + 1:]))
        r_2 = torch.dot(r_3.double(), input_x.double())
        return r_1[index], r_2


class PeriodFinder:
    def __init__(self, k):
        """
        Initialize the PeriodFinder class.

        Args:
            k (int): Number of most significant periods to find.
        """
        self.k = k

    def find_periods(self, signal):
        """
        Find the k most significant periods in the time series using FFT.

        Args:
            signal (torch.Tensor): 1D tensor representing the time series data.

        Returns:
            List[float]: The k most significant periods sorted by significance.
        """
        if not isinstance(signal, torch.Tensor):
            raise ValueError("Input time_series must be a torch.Tensor")

        if signal.dim() != 1:
            raise ValueError("Input time_series must be a 1D tensor")

        # Compute the FFT of the time series
        fft_result = torch.fft.fft(signal)

        # Compute the power spectrum (magnitude squared of FFT results)
        power_spectrum = torch.abs(fft_result) ** 2

        # Exclude the zero-frequency component
        power_spectrum[0] = 0

        # Get the frequencies corresponding to FFT components
        n = len(signal)
        frequencies = torch.fft.fftfreq(n)

        # Convert frequencies to periods (1/frequency)
        positive_frequencies = frequencies[frequencies > 0]
        periods = 1 / positive_frequencies

        # Get the corresponding power spectrum for positive frequencies
        positive_power_spectrum = power_spectrum[frequencies > 0]

        # Find the indices of the top k power spectrum values
        top_k_indices = torch.topk(positive_power_spectrum, self.k).indices

        # Retrieve the periods corresponding to the top k indices
        significant_periods = periods[top_k_indices]

        # Sort the periods by their power spectrum value (descending)
        sorted_periods = significant_periods[torch.argsort(positive_power_spectrum[top_k_indices], descending=True)]

        return sorted_periods.tolist()


class FullyConnectedModel(nn.Module):
    def __init__(self, input_size, output_size):
        """
        åˆå§‹åŒ–å…¨è¿æ¥ç½‘ç»œã€‚

        å‚æ•°ï¼š
        input_size (int): è¾“å…¥å±‚çš„å¤§å°ã€‚
        output_size (int): è¾“å‡ºå±‚çš„å¤§å°ã€‚
        """
        super(FullyConnectedModel, self).__init__()

        # å®šä¹‰å•å±‚å…¨è¿æ¥
        self.activation = nn.Tanh()
        self.fc = nn.Linear(input_size, output_size)

        # è‡ªå®šä¹‰å‚æ•°åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.fc.weight, nonlinearity='relu')  # ä½¿ç”¨Kaimingåˆå§‹åŒ–
        nn.init.zeros_(self.fc.bias)  # å°†åç½®åˆå§‹åŒ–ä¸º0

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­é€»è¾‘ã€‚

        å‚æ•°ï¼š
        x (torch.Tensor): è¾“å…¥å¼ é‡ã€‚

        è¿”å›ï¼š
        torch.Tensor: æ¨¡å‹è¾“å‡ºã€‚
        """
        x = self.fc(x)
        x = self.activation(x)
        return x


def calculate_elementwise_lcm(list1, list2):
    """
            è®¡ç®—ä¸¤ä¸ªå¼ é‡ä¹‹é—´æ¯å¯¹å…ƒç´ çš„æœ€å°å…¬å€æ•°
            :param: 1D PyTorch å¼ é‡
            :param: 1D PyTorch å¼ é‡
            :return: ä¸€ä¸ª 2D å¼ é‡ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ tensor1 å’Œ tensor2 ä¸­å¯¹åº”å…ƒç´ çš„æœ€å°å…¬å€æ•°
            """
    # ç¡®ä¿ä¸¤ä¸ªå¼ é‡æ˜¯æ•´å‹
    # å°†åˆ—è¡¨è½¬æ¢ä¸º PyTorch å¼ é‡
    tensor1 = torch.tensor(list1, dtype=torch.long)
    tensor2 = torch.tensor(list2, dtype=torch.long)

    # ä½¿ç”¨å¹¿æ’­æœºåˆ¶è®¡ç®—æ¯å¯¹å…ƒç´ çš„ GCD
    gcd = torch.gcd(tensor1[:, None], tensor2[None, :])

    # ä½¿ç”¨å…¬å¼ LCM(a, b) = abs(a * b) // GCD(a, b)
    lcm = torch.div((tensor1[:, None] * tensor2[None, :]).abs(), gcd, rounding_mode='trunc')

    # æ‰¾åˆ°æœ€å°å…¬å€æ•°åŠå…¶ç´¢å¼•
    min_lcm, flat_index = torch.min(lcm.flatten(), dim=0)  # å±•å¹³çŸ©é˜µå¹¶æ‰¾åˆ°æœ€å°å€¼åŠå…¶ç´¢å¼•
    min_i, min_j = divmod(flat_index.item(), lcm.size(1))  # è®¡ç®—äºŒç»´ç´¢å¼•

    return list1[min_i], list2[min_j], min_lcm.item()


class AdaptiveEncoding:
    def __init__(self):
        # åˆ›å»ºå‘¨æœŸæŸ¥æ‰¾å™¨å¯¹è±¡ (k=2 è¡¨ç¤ºå¯»æ‰¾å‰ 2 ä¸ªæœ€é‡è¦çš„å‘¨æœŸ)
        self.ad_finder = PeriodFinder(k=3)
        # åˆ›å»ºå¾ªç¯å‘é‡ç”Ÿæˆå™¨å¯¹è±¡ (factor=2 è¡¨ç¤ºç”Ÿæˆçš„å‘é‡ä¼šæŒ‰å› å­ 2 ç¼©æ”¾)
        self.ad_key = CyclicVectorGenerator(factor=1)
        # åˆ›å»ºå¾ªç¯å·ç§¯å¯¹è±¡å¹¶å¯¹å½’ä¸€åŒ–æ•°æ®è¿›è¡Œå¾ªç¯å·ç§¯
        self.ad_circular_conv = CircularConvolution1()

    def encode_process(self, input_x, input_y):
        # æ•°æ®é¢„å¤„ç†ï¼šå°† processed_trip_dict ä¸­çš„ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªè½¨è¿¹æ•°æ®å½’ä¸€åŒ–å¹¶è½¬æ¢ä¸ºå¼ é‡
        normalized_trip1 = torch.tensor(
            np.array(Data_preprocessing.normalized(input_x), dtype=float))  # ç¬¬ä¸€ä¸ªè½¨è¿¹æ•°æ®
        normalized_trip2 = torch.tensor(
            np.array(Data_preprocessing.normalized(input_y), dtype=float))  # ç¬¬äºŒä¸ªè½¨è¿¹æ•°æ®

        # æŸ¥æ‰¾ä¸¤ä¸ªè½¨è¿¹çš„ä¸»è¦å‘¨æœŸ
        top_period1_ = self.ad_finder.find_periods(normalized_trip1)
        top_period2_ = self.ad_finder.find_periods(normalized_trip2)
        top_period1, top_period2, min_gcd = calculate_elementwise_lcm(top_period1_, top_period2_)

        # è®¡ç®—ä¸¤ä¸ªå‘¨æœŸçš„æœ€å°å…¬å€æ•°å¹¶è¿”å›ç›¸åº”çš„å‘¨æœŸå’Œæœ€å°å…¬å€æ•°
        top_period1, top_period2, min_gcd = calculate_elementwise_lcm(top_period1_, top_period2_)

        # æ ¹æ®æœ€å°å‘¨æœŸç”Ÿæˆä¸¤ä¸ªå…³é”®å‘é‡
        key_vector, key_vector_whole = self.ad_key.generate_final_vector(length=len(normalized_trip1),
                                                                         period=int(min_gcd),
                                                                         num_vectors=2)

        cir_vector1, key_matrix1 = self.ad_circular_conv(key_vector_whole[0, :], normalized_trip1, 1)
        cir_vector2, key_matrix2 = self.ad_circular_conv(key_vector_whole[1, :], normalized_trip2, 1)
        cir_vector3 = cir_vector1 + cir_vector2
        return cir_vector3


class PeriodAdaptation(nn.Module):
    def __init__(self, compress_size):
        super(PeriodAdaptation, self).__init__()
        self.fc1 = nn.Linear(compress_size, 1)
        self.activation = nn.Sigmoid()

    def forward(self, data):
        device = self.fc1.weight.device  # ğŸ”§ è·å–è¯¥å±‚æ‰€åœ¨è®¾å¤‡
        data = data.to(device)  # âœ… æŠŠè¾“å…¥æ”¾åˆ°åŒè®¾å¤‡ä¸Š
        x = self.fc1(data)
        mean = self.activation(x)
        variance = self.r_mse(x, mean)
        return mean, variance

    @staticmethod
    def r_mse(x, mean):
        mse_b = torch.mean((x - mean) ** 2, dim=2, keepdim=True)
        x_b = torch.sqrt(mse_b)
        return x_b


class PeriodAdaptationOutput(nn.Module):
    def __init__(self, compress_size):
        super(PeriodAdaptationOutput, self).__init__()
        self.fc1 = nn.Linear(compress_size, 1)
        self.activation = nn.Sigmoid()

    def forward(self, data):
        x = self.fc1(data)
        mean = self.activation(x)
        variance = self.r_mse(x, mean)
        return mean, variance

    @staticmethod
    def r_mse(x, mean):
        mse_b = torch.mean((x - mean) ** 2, dim=1, keepdim=True)
        x_b = torch.sqrt(mse_b)
        return x_b


class ElementWiseProductLayer(nn.Module):
    def __init__(self, input_size, out_channel):
        super(ElementWiseProductLayer, self).__init__()
        # å®šä¹‰å¯è®­ç»ƒçš„å‚æ•° weights, å¤§å°ä¸è¾“å…¥ä¸€è‡´
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channel, kernel_size=1)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(p=0.1)  # 30% éšæœºä¸¢å¼ƒ
        self.conv2 = nn.Conv1d(in_channels=out_channel, out_channels=out_channel, kernel_size=1)
        self.linear2 = nn.Linear(input_size, input_size)

    def forward(self, x):
        # light weight
        x = self.conv1(x)
        x = self.gelu(x)
        # x = self.dropout(x)
        x_in = self.conv2(x)
        x = x + x_in
        x = self.linear2(x)
        with torch.no_grad():
            x_in = self.linear2(x_in)
        return x, x_in
